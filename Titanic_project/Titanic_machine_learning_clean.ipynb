{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import texttable as tt\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection \n",
    "from sklearn import linear_model \n",
    "from sklearn import svm\n",
    "from sklearn import neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv', index_col='PassengerId')\n",
    "del data.index.name # lets also remove this row with just the name on it to make things easier later\n",
    "data_test = pd.read_csv('data/test.csv', index_col='PassengerId')\n",
    "del data_test.index.name # lets also remove this row with just the name on it to make things easier later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass                                               Name  \\\n",
       "1         0       3                            Braund, Mr. Owen Harris   \n",
       "2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "3         1       3                             Heikkinen, Miss. Laina   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "1    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "2  female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "3  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is the data we have to work with:\n",
    "\n",
    "Survivied is our target variable we wish to predict, a value of 1 means survival, a value of 0 means death.\n",
    "\n",
    "Pclass is a proxy for socio-economic status the values are as so:\n",
    "- 1st = Upper\n",
    "- 2nd = Middle\n",
    "- 3rd = Lower\n",
    "\n",
    "SibSp quantifies how many siblings/spouses were aboard with the passenger.\n",
    "\n",
    "Parch quantifies how many parents/children were aboard with the passenger.\n",
    "\n",
    "Embarked codifies the port at which the passenger embarked as so:\n",
    "- C = Cherbourg\n",
    "- Q = Queenstown\n",
    "- S = Southampton\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering / data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use many machine learning algorithms we first have to do some preprocessing to get it into an appropriate format. We'll go through the features one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 1: Pclass\n",
    "\n",
    "First lets check for missing data entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of missing entries in training data = 0\n",
      "Num of missing entries in test data = 0\n"
     ]
    }
   ],
   "source": [
    "print('Num of missing entries in training data = {}'.format(sum(np.isnan(data.Pclass))))\n",
    "print('Num of missing entries in test data = {}'.format(sum(np.isnan(data_test.Pclass))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing entries here so we move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pclass is a categorical variable, these need to be encoded into binary variables. This is nessesary because of the way the algorithm interprets numbers. If we have a categorical feature that takes values 0, 1, 2, 3, 4 it assumes the higher numbers are 'better' (e.g. 4>3) even though they are arbitrary encodings, because ultimately it is calculating values/weights/parameters to be multiplied by these feature variables to give a term which enters into the linear regression. One common way to deal with this is one-hot-encoding, where a feature N takes values 0, 1, 2 for example we would generate 3 features which takes binary values 0 or 1. An example is shown below\n",
    "\n",
    "We have the original feature data:\n",
    "\n",
    "| Entry        | N          |\n",
    "| ------------ |:----------:|\n",
    "| 0            | 1          | \n",
    "| 1            | 2          |\n",
    "| 2            | 0          |\n",
    "| 3            | 1          |\n",
    "| 4            | 2          |\n",
    "| 5            | 0          |\n",
    "\n",
    "Which when encoded becomes:\n",
    "\n",
    "| Entry        | N==0       | N==1       | N==2       |\n",
    "| ------------ |:----------:|:----------:|:----------:|\n",
    "| 0            | 0          | 1          | 0          | \n",
    "| 1            | 0          | 0          | 1          | \n",
    "| 2            | 1          | 0          | 0          | \n",
    "| 3            | 0          | 1          | 0          | \n",
    "| 4            | 0          | 0          | 1          | \n",
    "| 5            | 1          | 0          | 0          | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on training data\n",
    "\n",
    "Pclass_lb = preprocessing.LabelBinarizer()\n",
    "Pclass_one_hot_encoded = Pclass_lb.fit(data['Pclass']) # one-hot encoding\n",
    "Pclass_one_hot_encoded = Pclass_lb.transform(data['Pclass']) # one-hot encoding\n",
    "\n",
    "dfOneHot_Encoded = pd.DataFrame(\n",
    "    Pclass_one_hot_encoded, \n",
    "    columns = [\"Pclass_\"+str(int(i+1)) for i in range(Pclass_one_hot_encoded.shape[1])],\n",
    "    index=data.index\n",
    "    ) # we now construct a dataframe out of this one-hot-encoded data\n",
    "\n",
    "# we now add our one-hot-encoded Embarked features\n",
    "data = pd.concat([data, dfOneHot_Encoded], axis=1)\n",
    "del(data['Pclass']) # and delete the original feature\n",
    "\n",
    "# on testing data\n",
    "\n",
    "Pclass_one_hot_encoded = Pclass_lb.transform(data_test['Pclass']) # one-hot encoding\n",
    "dfOneHot_Encoded = pd.DataFrame(\n",
    "    Pclass_one_hot_encoded, \n",
    "    columns = [\"Pclass_\"+str(int(i+1)) for i in range(Pclass_one_hot_encoded.shape[1])],\n",
    "    index=data_test.index\n",
    "    ) # we now construct a dataframe out of this one-hot-encoded data\n",
    "\n",
    "# we now add our one-hot-encoded Embarked features\n",
    "data_test = pd.concat([data_test, dfOneHot_Encoded], axis=1)\n",
    "del(data_test['Pclass']) # and delete the original feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 2: Name\n",
    "\n",
    "We will drop this feature as names are unlikely to effect survivability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.drop(labels=['Name'], axis=1, inplace=True)\n",
    "data_test.drop(labels=['Name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 3: Sex\n",
    "Sex can be encoded easily as a single binary feature as it only has 2 values, male or female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female'], dtype=object)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le_sex = preprocessing.LabelEncoder()\n",
    "le_sex.fit(data.Sex) # fits a value to each unique integer value of the feature variable sex\n",
    "data.Sex = le_sex.transform(data.Sex) # transform the data from labels to numeric\n",
    "\n",
    "data_test.Sex = le_sex.transform(data_test.Sex) # transform the data from labels to numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 4: Age\n",
    "\n",
    "Lets first check for missing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of missing entries in training data = 177\n",
      "Num of missing entries in test data = 86\n"
     ]
    }
   ],
   "source": [
    "print('Num of missing entries in training data = {}'.format(sum(np.isnan(data.Age))))\n",
    "print('Num of missing entries in test data = {}'.format(sum(np.isnan(data_test.Age))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have quite a lot of missing age data, lets just try replacing it with the median of the age data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputer = preprocessing.Imputer(strategy=\"median\", axis=0)\n",
    "data['Age'] = imputer.fit_transform(data['Age'].values.reshape(-1, 1))\n",
    "data_test['Age'] = imputer.transform(data_test['Age'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of missing entries in training data = 0\n",
      "Num of missing entries in test data = 0\n"
     ]
    }
   ],
   "source": [
    "print('Num of missing entries in training data = {}'.format(sum(np.isnan(data.Age))))\n",
    "print('Num of missing entries in test data = {}'.format(sum(np.isnan(data_test.Age))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age is our first continous feature. There are 2 approaches one can take to continuous data such as this, one is to leave it as continous data but perform feature scaling (make it a similar scale to other parameters), another is to discretise it into bins. The second approach *can* help the learning algorithm to pick up on general trends, such as young people and old people perhaps (don't know if this is true for this data) being more likely to die. However we don't know for sure which approach is best for this particular data and should test it rather than picking one based on a *hunch* or preconcieved notions of what approach is best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now I will leave this as a continous variable and perform feature scaling, later I will try the binning method and compare the results with the different learning algorithms to decide which approach is best for this data.\n",
    "\n",
    "Feature scaling makes it so the mean is 0 and the variance one, applying this to all continous variables means they are all on the same scale and thus will be weighted similarly and the parameter space can converge in minimising the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler_age = preprocessing.StandardScaler().fit(data['Age'].values.reshape(-1, 1))\n",
    "data['Age'] = scaler_age.transform(data['Age'].values.reshape(-1, 1))\n",
    "data_test['Age'] = scaler_age.transform(data_test['Age'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 5 & 6: SibSp and Parch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 2 parameters can be combined to parameterise the number of relatives aboard the ship, which seems like a sensible combination to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['NumRels'] = data['SibSp'] + data['Parch']\n",
    "data_test['NumRels'] = data_test['SibSp'] + data_test['Parch']\n",
    "data.drop(labels=['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "data_test.drop(labels=['SibSp', 'Parch'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of missing entries in training data = 0\n",
      "Num of missing entries in test data = 0\n"
     ]
    }
   ],
   "source": [
    "print('Num of missing entries in training data = {}'.format(sum(np.isnan(data.NumRels))))\n",
    "print('Num of missing entries in test data = {}'.format(sum(np.isnan(data_test.NumRels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now treat this new feature as a continuous variable rather than treating it as a catagorical variable which takes on discrete values, which is another valid approach. Therefore we perform feature scaling on this new continous variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajs3g11/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler_NumRels = preprocessing.StandardScaler().fit(data['NumRels'].values.reshape(-1, 1))\n",
    "data['NumRels'] = scaler_NumRels.transform(data['NumRels'].values.reshape(-1, 1))\n",
    "data_test['NumRels'] = scaler_NumRels.transform(data_test['NumRels'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 7: Ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just delete this information since they are unique ticket numbers and contain no relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop(labels=['Ticket'], axis=1, inplace=True)\n",
    "data_test.drop(labels=['Ticket'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 8: Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fare is another continous feature we want to feature scale, or could discretise alternatively,  here we feature scale and treat it as continous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of missing entries in training data = 0\n",
      "Num of missing entries in test data = 1\n"
     ]
    }
   ],
   "source": [
    "print('Num of missing entries in training data = {}'.format(sum(np.isnan(data.Fare))))\n",
    "print('Num of missing entries in test data = {}'.format(sum(np.isnan(data_test.Fare))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However before we do that we need to take care of the missing entry in the testing data. We will use the median of the training data to replace the missing entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputer = preprocessing.Imputer(strategy=\"median\", axis=0)\n",
    "imputer.fit(data['Fare'].values.reshape(-1, 1))\n",
    "data_test['Fare'] = imputer.transform(data_test['Fare'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler_Fare = preprocessing.StandardScaler().fit(data['Fare'].values.reshape(-1, 1))\n",
    "data['Fare'] = scaler_Fare.transform(data['Fare'].values.reshape(-1, 1))\n",
    "data_test['Fare'] = scaler_Fare.transform(data_test['Fare'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 9: Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of missing entries in training data = 687\n",
      "Num of missing entries in test data = 327\n"
     ]
    }
   ],
   "source": [
    "print('Num of missing entries in training data = {}'.format(sum(pd.isnull(data['Cabin']))))\n",
    "print('Num of missing entries in test data = {}'.format(sum(pd.isnull(data_test['Cabin']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may well be some useful information here about the position on the ship of the passanger's cabins but there are so many missing values that we will just delete this feature for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop(labels=['Cabin'], axis=1, inplace=True)\n",
    "data_test.drop(labels=['Cabin'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 10: Embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the PClass data we want to encode this categorical feature into many binary features, we once again you one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of missing entries in training data = 2\n",
      "Num of missing entries in test data = 0\n"
     ]
    }
   ],
   "source": [
    "print('Num of missing entries in training data = {}'.format(sum(pd.isnull(data.Embarked))))\n",
    "print('Num of missing entries in test data = {}'.format(sum(pd.isnull(data_test.Embarked))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there's only 2 null values in the training data we'll drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>NumRels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.565736</td>\n",
       "      <td>-0.502445</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.059160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.663861</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>-0.486337</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.560975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Sex       Age      Fare Embarked  Pclass_1  Pclass_2  Pclass_3  \\\n",
       "1         0    1 -0.565736 -0.502445        S         0         0         1   \n",
       "2         1    0  0.663861  0.786845        C         1         0         0   \n",
       "5         0    1  0.433312 -0.486337        S         0         0         1   \n",
       "\n",
       "    NumRels  \n",
       "1  0.059160  \n",
       "2  0.059160  \n",
       "5 -0.560975  "
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:5].drop([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 830]\n"
     ]
    }
   ],
   "source": [
    "indexes_of_nulls = [i for i, x in zip(data.index, pd.isnull(data.Embarked)) if x]\n",
    "print(indexes_of_nulls)\n",
    "data.drop(np.array(indexes_of_nulls), inplace=True) # remove examples with null values for embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on training data\n",
    "\n",
    "Embarked_lb = preprocessing.LabelBinarizer()\n",
    "Embarked_one_hot_encoded = Embarked_lb.fit(data['Embarked']) # one-hot encoding\n",
    "Embarked_one_hot_encoded = Embarked_lb.transform(data['Embarked']) # one-hot encoding\n",
    "\n",
    "dfOneHot_Encoded = pd.DataFrame(\n",
    "    Embarked_one_hot_encoded, \n",
    "    columns = [\"Embarked_\"+str(int(i)) for i in range(Embarked_one_hot_encoded.shape[1])],\n",
    "    index=data.index\n",
    "    ) # we now construct a dataframe out of this one-hot-encoded data\n",
    "\n",
    "# we now add our one-hot-encoded Embarked features\n",
    "data = pd.concat([data, dfOneHot_Encoded], axis=1)\n",
    "del(data['Embarked']) # and delete the original feature\n",
    "\n",
    "# on testing data\n",
    "\n",
    "Embarked_one_hot_encoded = Embarked_lb.transform(data_test['Embarked']) # one-hot encoding\n",
    "dfOneHot_Encoded = pd.DataFrame(\n",
    "    Embarked_one_hot_encoded, \n",
    "    columns = [\"Embarked_\"+str(int(i)) for i in range(Embarked_one_hot_encoded.shape[1])],\n",
    "    index=data_test.index\n",
    "    ) # we now construct a dataframe out of this one-hot-encoded data\n",
    "\n",
    "# we now add our one-hot-encoded Embarked features\n",
    "data_test = pd.concat([data_test, dfOneHot_Encoded], axis=1)\n",
    "del(data_test['Embarked']) # and delete the original feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>NumRels</th>\n",
       "      <th>Embarked_0</th>\n",
       "      <th>Embarked_1</th>\n",
       "      <th>Embarked_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.565736</td>\n",
       "      <td>-0.502445</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.059160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.663861</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059160</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258337</td>\n",
       "      <td>-0.488854</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.560975</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>0.420730</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>-0.486337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.560975</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Sex       Age      Fare  Pclass_1  Pclass_2  Pclass_3   NumRels  \\\n",
       "1         0    1 -0.565736 -0.502445         0         0         1  0.059160   \n",
       "2         1    0  0.663861  0.786845         1         0         0  0.059160   \n",
       "3         1    0 -0.258337 -0.488854         0         0         1 -0.560975   \n",
       "4         1    0  0.433312  0.420730         1         0         0  0.059160   \n",
       "5         0    1  0.433312 -0.486337         0         0         1 -0.560975   \n",
       "\n",
       "   Embarked_0  Embarked_1  Embarked_2  \n",
       "1           0           0           1  \n",
       "2           1           0           0  \n",
       "3           0           0           1  \n",
       "4           0           0           1  \n",
       "5           0           0           1  "
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now seperate our feature and target variables for the training data and begin to train some machine learning classifiers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data.drop([\"Survived\"], axis=1)\n",
    "y = data[\"Survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.10) # we will also hold back 10% of our data as test data to appraise the fitted and tuned models after validation before applying them to our Kaggle test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Methods\n",
    "\n",
    "Before we start training models it's important to consider how we will assess the accuracy of the predictions it makes. Assessing this on the data we used to train it is a very bad practise and is called the *resubstitution error*. This is because you cannot know if you are overfitting or not if you assess your model on the training data.\n",
    "\n",
    "So how do we assess the quality of our learning algorithms? There are a few approaches called Validation (because they validate our model). It is also important to have a 3rd set of test data. The purpose of this is that we train many learning algorithms on the trianing data and validate the performance of them on the validation data to pick which one to use. The performance of the algorithm on the validation data is no-longer a fair assessment of the accuracy, as the algorithm was picked for the fact it performed well on that data. We now assess the accuracy of the model fitted with training data and tuned (as in, we pick the algorithm we are using and the hyperparameters for that algorithm) by validation data on data it has never seen, i.e. test data. This should give us a fair assessment of the actual accuracy of the algorithm.\n",
    "\n",
    "#### Holdout\n",
    "The first approach is simple, we simply split the data into a training set and validation (also called cross-validation) set, also called the holdout set (as we hold it out from the training set). We train the model on the training data and validate the model on the valdiation data. In this case it is important to be careful that the sets contain similar occurances of different classes in the data, this process of equal distribution of classes is called *stratification*.\n",
    "\n",
    "#### K-Fold Cross-Validation\n",
    "The problem with this first approach is that we lost part of the training data we'd like our model to learn from, by reducing the data in this way we are forced to use a lower bias model (more data => higher variance models can be fit => higher accuracy can be achieved).\n",
    "\n",
    "In K-Fold cross validation the data is divided into k subsets and we perform the hold-out method k times, such that each time, one of the k subsets is used as the validation set and the other k-1 subsets are put together to form a training set. The error estimation is averaged over all k trials to get total effectiveness of our model. In this scheme every data point gets to be in a validation set exactly once, and every data point gets to be in a training set k-1 times. \n",
    "\n",
    "#### Stratified K-Fold Cross Validation\n",
    "This is the same as above but care is taken such that each fold contains approximately the same percentage of samples of each target class as the complete set, or in case of prediction problems, the mean response value is approximately equal in all the folds.\n",
    "\n",
    "#### Leave-$P$-out Cross Validation (or Leave-One-Out Cross Validation [LOOCV])\n",
    "This approach is similar to k-folds cross validation. We leave $P$ points out of the training data and validate with these p points. We then repeat this for all possible combinations. Commonly $P$ is set to $1$ and this becomes ***Leave-one-out Cross Validation***. This is generally preferred as it is easy to compute all combinations (there are $m$, where m = number of training examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now use scikit learn to fit a regularised logistic regression \n",
    "# model with each of the feature variables being linear\n",
    "# setting fit_intercept=True fits the Theta_0 term - an intercept term\n",
    "\n",
    "lr_model = linear_model.LogisticRegression(fit_intercept=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets use stratified K-folds cross validation with 10 folds to evaluate our algorithm - can use sklearn.model_selection.cross_val_score for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_splits = 10\n",
    "kfold = model_selection.KFold(n_splits=N_splits)\n",
    "\n",
    "scoring = 'accuracy'\n",
    "score = model_selection.cross_val_score(lr_model, X_train, y_train, cv=kfold, n_jobs=1, scoring=scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80249999999999999"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean() # we get an average accuracy of 80% over 10 folds with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now train our model, which we've confirmed should give ~80% accuracy, on our entire training data set \n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.797752808988764"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and test it on our test data we have held back from the training/validation set\n",
    "lr_model.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also get ~80% accuracy, so this suggests the model is fitted correctly and is performing well, we now predict on Kaggle's test data and save the prediction to submit to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = lr_model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('prediction_submission_LR.csv', 'w') as file:\n",
    "    print('PassengerId,Survived', file=file)\n",
    "    for i, id_ in enumerate(data_test.index):\n",
    "        print('{},{}'.format(id_, prediction[i]), file=file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we submit this result to Kaggle we get a reported accuracy of 76.076% which is significantly lower than we evaluated on our held back test data, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
